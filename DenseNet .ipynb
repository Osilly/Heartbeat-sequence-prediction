{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18eb7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3daaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 64, 48),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=4, memory_efficient=False):\n",
    " \n",
    "        super(DenseNet, self).__init__()\n",
    " \n",
    "        # 首层卷积层\n",
    "        self.Net = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv1d(1, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm1d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool1d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    " \n",
    "        # 构建DenseBlock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config): #构建4个DenseBlock\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.Net.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,  #每个DenseBlock后跟一个TransitionLayer\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.Net.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    " \n",
    "        # Final batch norm\n",
    "        self.Net.add_module('norm5', nn.BatchNorm1d(num_features))\n",
    " \n",
    "        # Linear layer\n",
    "        self.Classifier = nn.Linear(num_features, num_classes) #构建分类器\n",
    " \n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    " \n",
    "    def forward(self, x):\n",
    "        features = self.Net(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool1d(out, 1)\n",
    "#         print(out.shape)\n",
    "        out = torch.flatten(out, 1)\n",
    "#         print(out.shape)\n",
    "        out = self.Classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9bc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    " \n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1) #将之前的层拼接在一起，并且按行展开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064bd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "    return bn_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9d68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm1d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv1d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm1d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    " \n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3188a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm1d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv1d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool1d(kernel_size=2, stride=2)) #尺寸减少一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e4210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return torch.Tensor(data),label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def train_dataset(train_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    # 处理训练数据\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    for item in train.values:\n",
    "        # train_data\n",
    "        arr = np.array([float(i) for i in item[1].split(',')])\n",
    "        arr.resize((1,205))\n",
    "        train_data.append(arr)\n",
    "        #train_label\n",
    "#         arr = np.zeros((4))\n",
    "#         arr[int(item[2])]=1.0\n",
    "#         train_label.append(arr)\n",
    "        train_label.append(item[2])\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    data = TrainDataset(train_data, train_label)\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    validdate_size = int(len(data)) - train_size\n",
    "    traindata, validdata = torch.utils.data.random_split(data, [train_size, validdate_size])\n",
    "    return traindata, validdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291306bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        return torch.Tensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def test_dataset(test_path):\n",
    "    test = pd.read_csv(test_path)\n",
    "    # 处理训练数据\n",
    "    test_data = []\n",
    "    for item in test.values:\n",
    "        # train_data\n",
    "        arr = np.array([float(i) for i in item[1].split(',')])\n",
    "        arr.resize((1,205))\n",
    "        test_data.append(arr)\n",
    "    \n",
    "    testdata = TestDataset(test_data)\n",
    "    return testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6e63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(out, label):\n",
    "    total = out.shape[0]\n",
    "    _, pred_label = out.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6aa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainiter, validiter, num_epochs, optimizer, criterion):\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available else 'cpu')\n",
    "    net = net.to(device)\n",
    "    print(\"train start!\")\n",
    "    for epoch in range(num_epochs):\n",
    "        net = net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for data, label in trainiter:\n",
    "#             # 将装有tensor的list转换为tensor\n",
    "#             data = torch.stack(data,1)\n",
    "            \n",
    "            data = data.to(device)\n",
    "            label = label.to(device, dtype=torch.int64)\n",
    "            # 前向传播\n",
    "            out = net(data)\n",
    "            loss = criterion(out, label)\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 计算loss和accuracy\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(out, label)\n",
    "        \n",
    "        if validiter is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for data, label in validiter:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device, dtype=torch.int64)\n",
    "                out = net(data)\n",
    "                loss = criterion(out, label)\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(out, label)\n",
    "                \n",
    "            print(\"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                    % (epoch, train_loss / len(trainiter),train_acc / len(trainiter), \n",
    "                       valid_loss / len(validiter),valid_acc / len(validiter)))\n",
    "        else:\n",
    "            print(\"Epoch %d. Train Loss: %f, Train Acc: %f, \"\n",
    "                   %(epoch, train_loss / len(trainiter),train_acc / len(trainiter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642ab02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgq/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.265222, Train Acc: 0.935275, Valid Loss: 0.078050, Valid Acc: 0.976800, \n",
      "Epoch 1. Train Loss: 0.074887, Train Acc: 0.976575, Valid Loss: 0.055833, Valid Acc: 0.982400, \n",
      "Epoch 2. Train Loss: 0.053513, Train Acc: 0.983075, Valid Loss: 0.067623, Valid Acc: 0.980350, \n",
      "Epoch 3. Train Loss: 0.041547, Train Acc: 0.986225, Valid Loss: 0.054731, Valid Acc: 0.982450, \n",
      "Epoch 4. Train Loss: 0.033894, Train Acc: 0.988775, Valid Loss: 0.044163, Valid Acc: 0.986250, \n",
      "Epoch 5. Train Loss: 0.027505, Train Acc: 0.990725, Valid Loss: 0.040489, Valid Acc: 0.987300, \n",
      "Epoch 6. Train Loss: 0.021115, Train Acc: 0.993038, Valid Loss: 0.039946, Valid Acc: 0.987700, \n",
      "Epoch 7. Train Loss: 0.017446, Train Acc: 0.994000, Valid Loss: 0.033834, Valid Acc: 0.990200, \n",
      "Epoch 8. Train Loss: 0.013361, Train Acc: 0.995613, Valid Loss: 0.038241, Valid Acc: 0.989200, \n",
      "Epoch 9. Train Loss: 0.014314, Train Acc: 0.995325, Valid Loss: 0.036232, Valid Acc: 0.989700, \n",
      "Epoch 10. Train Loss: 0.011026, Train Acc: 0.996125, Valid Loss: 0.036963, Valid Acc: 0.988850, \n",
      "Epoch 11. Train Loss: 0.008375, Train Acc: 0.997150, Valid Loss: 0.031986, Valid Acc: 0.990750, \n",
      "Epoch 12. Train Loss: 0.006845, Train Acc: 0.997700, Valid Loss: 0.037482, Valid Acc: 0.990300, \n",
      "Epoch 13. Train Loss: 0.005472, Train Acc: 0.997913, Valid Loss: 0.034660, Valid Acc: 0.991300, \n",
      "Epoch 14. Train Loss: 0.004348, Train Acc: 0.998688, Valid Loss: 0.033174, Valid Acc: 0.992200, \n",
      "Epoch 15. Train Loss: 0.003060, Train Acc: 0.999050, Valid Loss: 0.035445, Valid Acc: 0.991150, \n",
      "Epoch 16. Train Loss: 0.003366, Train Acc: 0.998888, Valid Loss: 0.037507, Valid Acc: 0.991000, \n",
      "Epoch 17. Train Loss: 0.006732, Train Acc: 0.997838, Valid Loss: 0.048558, Valid Acc: 0.988400, \n",
      "Epoch 18. Train Loss: 0.004525, Train Acc: 0.998650, Valid Loss: 0.038188, Valid Acc: 0.990800, \n",
      "Epoch 19. Train Loss: 0.002936, Train Acc: 0.999163, Valid Loss: 0.035973, Valid Acc: 0.992650, \n",
      "Epoch 20. Train Loss: 0.001561, Train Acc: 0.999563, Valid Loss: 0.034973, Valid Acc: 0.992300, \n",
      "Epoch 21. Train Loss: 0.000776, Train Acc: 0.999800, Valid Loss: 0.035754, Valid Acc: 0.992450, \n",
      "Epoch 22. Train Loss: 0.000424, Train Acc: 0.999963, Valid Loss: 0.036219, Valid Acc: 0.992450, \n",
      "Epoch 23. Train Loss: 0.000193, Train Acc: 0.999988, Valid Loss: 0.037082, Valid Acc: 0.992350, \n",
      "Epoch 24. Train Loss: 0.000124, Train Acc: 0.999988, Valid Loss: 0.036546, Valid Acc: 0.992550, \n",
      "Epoch 25. Train Loss: 0.000085, Train Acc: 1.000000, Valid Loss: 0.036758, Valid Acc: 0.992400, \n",
      "Epoch 26. Train Loss: 0.000066, Train Acc: 1.000000, Valid Loss: 0.037198, Valid Acc: 0.992350, \n",
      "Epoch 27. Train Loss: 0.000049, Train Acc: 1.000000, Valid Loss: 0.038395, Valid Acc: 0.992250, \n",
      "Epoch 28. Train Loss: 0.000049, Train Acc: 1.000000, Valid Loss: 0.038347, Valid Acc: 0.992400, \n",
      "Epoch 29. Train Loss: 0.000047, Train Acc: 1.000000, Valid Loss: 0.039289, Valid Acc: 0.992400, \n",
      "Epoch 30. Train Loss: 0.000042, Train Acc: 1.000000, Valid Loss: 0.038519, Valid Acc: 0.992300, \n",
      "Epoch 31. Train Loss: 0.000119, Train Acc: 0.999975, Valid Loss: 0.038821, Valid Acc: 0.992550, \n",
      "Epoch 32. Train Loss: 0.000047, Train Acc: 1.000000, Valid Loss: 0.039541, Valid Acc: 0.992700, \n",
      "Epoch 33. Train Loss: 0.000039, Train Acc: 1.000000, Valid Loss: 0.039450, Valid Acc: 0.992600, \n",
      "Epoch 34. Train Loss: 0.000032, Train Acc: 1.000000, Valid Loss: 0.039372, Valid Acc: 0.992750, \n",
      "Epoch 35. Train Loss: 0.000035, Train Acc: 1.000000, Valid Loss: 0.039204, Valid Acc: 0.992700, \n",
      "Epoch 36. Train Loss: 0.000032, Train Acc: 1.000000, Valid Loss: 0.039470, Valid Acc: 0.992650, \n",
      "Epoch 37. Train Loss: 0.000025, Train Acc: 1.000000, Valid Loss: 0.039661, Valid Acc: 0.992650, \n",
      "Epoch 38. Train Loss: 0.000026, Train Acc: 1.000000, Valid Loss: 0.039450, Valid Acc: 0.992750, \n",
      "Epoch 39. Train Loss: 0.000034, Train Acc: 1.000000, Valid Loss: 0.039957, Valid Acc: 0.992600, \n",
      "Epoch 40. Train Loss: 0.000019, Train Acc: 1.000000, Valid Loss: 0.040115, Valid Acc: 0.992700, \n",
      "Epoch 41. Train Loss: 0.000025, Train Acc: 1.000000, Valid Loss: 0.040782, Valid Acc: 0.992650, \n",
      "Epoch 42. Train Loss: 0.000041, Train Acc: 0.999988, Valid Loss: 0.040147, Valid Acc: 0.992500, \n",
      "Epoch 43. Train Loss: 0.000022, Train Acc: 1.000000, Valid Loss: 0.039726, Valid Acc: 0.992600, \n",
      "Epoch 44. Train Loss: 0.000022, Train Acc: 1.000000, Valid Loss: 0.039788, Valid Acc: 0.992650, \n",
      "Epoch 45. Train Loss: 0.000020, Train Acc: 1.000000, Valid Loss: 0.040709, Valid Acc: 0.992750, \n",
      "Epoch 46. Train Loss: 0.000014, Train Acc: 1.000000, Valid Loss: 0.040468, Valid Acc: 0.992500, \n",
      "Epoch 47. Train Loss: 0.000017, Train Acc: 1.000000, Valid Loss: 0.041059, Valid Acc: 0.992650, \n",
      "Epoch 48. Train Loss: 0.000027, Train Acc: 1.000000, Valid Loss: 0.040549, Valid Acc: 0.992500, \n",
      "Epoch 49. Train Loss: 0.000018, Train Acc: 1.000000, Valid Loss: 0.040598, Valid Acc: 0.992650, \n"
     ]
    }
   ],
   "source": [
    "# train1(学习率为0.1)\n",
    "if __name__ == '__main__':\n",
    "    net = DenseNet()   \n",
    "    batch_size = 200 \n",
    "    traindata, validdata = train_dataset('train.csv')\n",
    "    trainiter = DataLoader(traindata, batch_size=batch_size,shuffle=True)\n",
    "    validiter = DataLoader(validdata, batch_size=batch_size,shuffle=True)\n",
    "    num_epochs = 50\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-1) #随机梯度下降\n",
    "    criterion = nn.CrossEntropyLoss() #loss为交叉熵\n",
    "    \n",
    "    train(net, trainiter, validiter, num_epochs, optimizer, criterion)\n",
    "    torch.save(net, 'Densenet264.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f47b2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295.90491063204126\n"
     ]
    }
   ],
   "source": [
    "# 计算验证集分数\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    model = torch.load('Densenet264.pth')\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 转为test模式\n",
    "    batch_size = 200\n",
    "#     print(iter(testiter).next().shape)\n",
    "    result = []\n",
    "    result_label = []\n",
    "    for data, label in validiter:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pre = F.softmax(out, 1)\n",
    "        pre = pre.to('cpu')\n",
    "#         print(pre)\n",
    "        result.append(pre)\n",
    "        result_label.append(label)\n",
    "    \n",
    "#     print(result_label)\n",
    "    result = torch.stack(result, 0) #按照轴0将list转换为tensor\n",
    "    result = np.array(result)\n",
    "    result = result.reshape((20000,4))\n",
    "    result_label = torch.stack(result_label, 0) #按照轴0将list转换为tensor\n",
    "    result_label = np.array(result_label)\n",
    "    result_label = result_label.reshape((20000))\n",
    "    thr = [0.8, 0.45, 0.8, 0.8]\n",
    "    for x in result:\n",
    "        for i in [1, 2, 3, 0]:\n",
    "            if x[i] > thr[i]:\n",
    "                x[0:i] = 0\n",
    "                x[i+1:4] = 0\n",
    "                x[i] = 1\n",
    "\n",
    "    num = 0\n",
    "    i = 0\n",
    "    for x in result:\n",
    "        ans = 0\n",
    "        x[int(result_label[i])] -= 1.0\n",
    "        i += 1\n",
    "        ans = sum(abs(x))\n",
    "        num += ans\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "999018e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  label_0  label_1  label_2  label_3\n",
      "0      100000      1.0      0.0      0.0      0.0\n",
      "1      100001      0.0      0.0      1.0      0.0\n",
      "2      100002      0.0      0.0      0.0      1.0\n",
      "3      100003      1.0      0.0      0.0      0.0\n",
      "4      100004      1.0      0.0      0.0      0.0\n",
      "...       ...      ...      ...      ...      ...\n",
      "19995  119995      1.0      0.0      0.0      0.0\n",
      "19996  119996      1.0      0.0      0.0      0.0\n",
      "19997  119997      0.0      0.0      1.0      0.0\n",
      "19998  119998      1.0      0.0      0.0      0.0\n",
      "19999  119999      1.0      0.0      0.0      0.0\n",
      "\n",
      "[20000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "    model = torch.load('Densenet264.pth')\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 转为test模式\n",
    "    batch_size = 200\n",
    "    testdata = test_dataset('testA.csv')\n",
    "    testiter = DataLoader(testdata, batch_size=batch_size,shuffle=False) # 一定要定义为False!\n",
    "#     print(iter(testiter).next().shape)\n",
    "    result = []\n",
    "    for data in testiter:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pre = F.softmax(out, 1)\n",
    "        pre = pre.to('cpu')\n",
    "#         print(pre)\n",
    "        result.append(pre)\n",
    "    result = torch.stack(result, 0) #按照轴0将list转换为tensor\n",
    "    # 进行数据的后处理，准备提交数据(设置阈值)\n",
    "    result = np.array(result)\n",
    "    result = result.reshape((20000,4))\n",
    "    thr = [0.8, 0.45, 0.8, 0.8]\n",
    "    for x in result:\n",
    "        for i in [1, 2, 3, 0]:\n",
    "            if x[i] > thr[i]:\n",
    "                x[0:i] = 0\n",
    "                x[i+1:4] = 0\n",
    "                x[i] = 1\n",
    "\n",
    "    id =np.arange(100000,120000)\n",
    "    df = DataFrame(result, columns=['label_0','label_1','label_2','label_3'])\n",
    "    df.insert(loc=0, column='id', value=id, allow_duplicates=False) \n",
    "    df.to_csv(\"submit.csv\", index_label=\"id\", index = False)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a0ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

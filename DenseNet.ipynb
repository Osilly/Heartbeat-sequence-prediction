{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18eb7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3daaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=4, memory_efficient=False):\n",
    " \n",
    "        super(DenseNet, self).__init__()\n",
    " \n",
    "        # 首层卷积层\n",
    "        self.Net = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv1d(1, num_init_features, kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm1d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool1d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    " \n",
    "        # 构建DenseBlock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config): #构建4个DenseBlock\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.Net.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,  #每个DenseBlock后跟一个TransitionLayer\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.Net.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    " \n",
    "        # Final batch norm\n",
    "        self.Net.add_module('norm5', nn.BatchNorm1d(num_features))\n",
    " \n",
    "        # Linear layer\n",
    "        self.Classifier = nn.Linear(num_features, num_classes) #构建分类器\n",
    " \n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    " \n",
    "    def forward(self, x):\n",
    "        features = self.Net(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool1d(out, 1)\n",
    "#         print(out.shape)\n",
    "        out = torch.flatten(out, 1)\n",
    "#         print(out.shape)\n",
    "        out = self.Classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9bc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    " \n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1) #将之前的层拼接在一起，并且按行展开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064bd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "    return bn_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9d68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm1d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv1d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm1d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    " \n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3188a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm1d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv1d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool1d(kernel_size=2, stride=2)) #尺寸减少一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e4210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return torch.Tensor(data),label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def train_dataset(train_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    # 处理训练数据\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    for item in train.values:\n",
    "        # train_data\n",
    "        arr = np.array([float(i) for i in item[1].split(',')])\n",
    "        arr.resize((1,205))\n",
    "        train_data.append(arr)\n",
    "        #train_label\n",
    "#         arr = np.zeros((4))\n",
    "#         arr[int(item[2])]=1.0\n",
    "#         train_label.append(arr)\n",
    "        train_label.append(item[2])\n",
    "    \n",
    "    # 分割训练集和验证集\n",
    "    data = TrainDataset(train_data, train_label)\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    validdate_size = int(len(data)) - train_size\n",
    "    traindata, validdata = torch.utils.data.random_split(data, [train_size, validdate_size])\n",
    "    return traindata, validdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291306bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        return torch.Tensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def test_dataset(test_path):\n",
    "    test = pd.read_csv(test_path)\n",
    "    # 处理训练数据\n",
    "    test_data = []\n",
    "    for item in test.values:\n",
    "        # train_data\n",
    "        arr = np.array([float(i) for i in item[1].split(',')])\n",
    "        arr.resize((1,205))\n",
    "        test_data.append(arr)\n",
    "    \n",
    "    testdata = TestDataset(test_data)\n",
    "    return testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6e63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(out, label):\n",
    "    total = out.shape[0]\n",
    "    _, pred_label = out.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6aa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainiter, validiter, num_epochs, optimizer, criterion):\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available else 'cpu')\n",
    "    net = net.to(device)\n",
    "    print(\"train start!\")\n",
    "    for epoch in range(num_epochs):\n",
    "        net = net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for data, label in trainiter:\n",
    "#             # 将装有tensor的list转换为tensor\n",
    "#             data = torch.stack(data,1)\n",
    "            \n",
    "            data = data.to(device)\n",
    "            label = label.to(device, dtype=torch.int64)\n",
    "            # 前向传播\n",
    "            out = net(data)\n",
    "            loss = criterion(out, label)\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 计算loss和accuracy\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(out, label)\n",
    "        \n",
    "        if validiter is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for data, label in validiter:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device, dtype=torch.int64)\n",
    "                out = net(data)\n",
    "                loss = criterion(out, label)\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(out, label)\n",
    "                \n",
    "            print(\"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                    % (epoch, train_loss / len(trainiter),train_acc / len(trainiter), \n",
    "                       valid_loss / len(validiter),valid_acc / len(validiter)))\n",
    "        else:\n",
    "            print(\"Epoch %d. Train Loss: %f, Train Acc: %f, \"\n",
    "                   %(epoch, train_loss / len(trainiter),train_acc / len(trainiter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642ab02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgq/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.411726, Train Acc: 0.877500, Valid Loss: 0.160583, Valid Acc: 0.954100, \n",
      "Epoch 1. Train Loss: 0.116784, Train Acc: 0.965850, Valid Loss: 0.102337, Valid Acc: 0.971000, \n",
      "Epoch 2. Train Loss: 0.079956, Train Acc: 0.976575, Valid Loss: 0.079910, Valid Acc: 0.976700, \n",
      "Epoch 3. Train Loss: 0.059991, Train Acc: 0.982112, Valid Loss: 0.069446, Valid Acc: 0.978950, \n",
      "Epoch 4. Train Loss: 0.045494, Train Acc: 0.986437, Valid Loss: 0.065891, Valid Acc: 0.981200, \n",
      "Epoch 5. Train Loss: 0.037291, Train Acc: 0.988850, Valid Loss: 0.057661, Valid Acc: 0.983650, \n",
      "Epoch 6. Train Loss: 0.028208, Train Acc: 0.991675, Valid Loss: 0.056878, Valid Acc: 0.983350, \n",
      "Epoch 7. Train Loss: 0.022052, Train Acc: 0.993775, Valid Loss: 0.059002, Valid Acc: 0.984100, \n",
      "Epoch 8. Train Loss: 0.017245, Train Acc: 0.995525, Valid Loss: 0.053126, Valid Acc: 0.985450, \n",
      "Epoch 9. Train Loss: 0.012162, Train Acc: 0.996987, Valid Loss: 0.054779, Valid Acc: 0.985850, \n",
      "Epoch 10. Train Loss: 0.008059, Train Acc: 0.998375, Valid Loss: 0.050549, Valid Acc: 0.986900, \n",
      "Epoch 11. Train Loss: 0.005759, Train Acc: 0.999037, Valid Loss: 0.051152, Valid Acc: 0.987350, \n",
      "Epoch 12. Train Loss: 0.003910, Train Acc: 0.999612, Valid Loss: 0.051075, Valid Acc: 0.987700, \n",
      "Epoch 13. Train Loss: 0.002310, Train Acc: 0.999863, Valid Loss: 0.049696, Valid Acc: 0.988500, \n",
      "Epoch 14. Train Loss: 0.001596, Train Acc: 0.999938, Valid Loss: 0.051152, Valid Acc: 0.988300, \n",
      "Epoch 15. Train Loss: 0.001273, Train Acc: 0.999987, Valid Loss: 0.051370, Valid Acc: 0.988000, \n",
      "Epoch 16. Train Loss: 0.001003, Train Acc: 1.000000, Valid Loss: 0.050823, Valid Acc: 0.988650, \n",
      "Epoch 17. Train Loss: 0.000867, Train Acc: 0.999987, Valid Loss: 0.050756, Valid Acc: 0.988550, \n",
      "Epoch 18. Train Loss: 0.000756, Train Acc: 1.000000, Valid Loss: 0.051824, Valid Acc: 0.988550, \n",
      "Epoch 19. Train Loss: 0.000662, Train Acc: 1.000000, Valid Loss: 0.051086, Valid Acc: 0.988550, \n",
      "Epoch 20. Train Loss: 0.000566, Train Acc: 1.000000, Valid Loss: 0.052220, Valid Acc: 0.988650, \n",
      "Epoch 21. Train Loss: 0.000528, Train Acc: 1.000000, Valid Loss: 0.052459, Valid Acc: 0.988600, \n",
      "Epoch 22. Train Loss: 0.000486, Train Acc: 1.000000, Valid Loss: 0.052606, Valid Acc: 0.988650, \n",
      "Epoch 23. Train Loss: 0.000449, Train Acc: 1.000000, Valid Loss: 0.053324, Valid Acc: 0.988550, \n",
      "Epoch 24. Train Loss: 0.000400, Train Acc: 1.000000, Valid Loss: 0.053856, Valid Acc: 0.988600, \n",
      "Epoch 25. Train Loss: 0.000360, Train Acc: 1.000000, Valid Loss: 0.053797, Valid Acc: 0.988750, \n",
      "Epoch 26. Train Loss: 0.000337, Train Acc: 1.000000, Valid Loss: 0.054381, Valid Acc: 0.988550, \n",
      "Epoch 27. Train Loss: 0.000322, Train Acc: 1.000000, Valid Loss: 0.054192, Valid Acc: 0.988600, \n",
      "Epoch 28. Train Loss: 0.000317, Train Acc: 1.000000, Valid Loss: 0.054377, Valid Acc: 0.988700, \n",
      "Epoch 29. Train Loss: 0.000279, Train Acc: 1.000000, Valid Loss: 0.054825, Valid Acc: 0.988600, \n",
      "Epoch 30. Train Loss: 0.000267, Train Acc: 1.000000, Valid Loss: 0.054914, Valid Acc: 0.988700, \n",
      "Epoch 31. Train Loss: 0.000257, Train Acc: 1.000000, Valid Loss: 0.055487, Valid Acc: 0.988700, \n",
      "Epoch 32. Train Loss: 0.000248, Train Acc: 1.000000, Valid Loss: 0.056018, Valid Acc: 0.988750, \n",
      "Epoch 33. Train Loss: 0.000238, Train Acc: 1.000000, Valid Loss: 0.055514, Valid Acc: 0.988800, \n",
      "Epoch 34. Train Loss: 0.000220, Train Acc: 1.000000, Valid Loss: 0.056026, Valid Acc: 0.988600, \n",
      "Epoch 35. Train Loss: 0.000220, Train Acc: 1.000000, Valid Loss: 0.056065, Valid Acc: 0.989000, \n",
      "Epoch 36. Train Loss: 0.000200, Train Acc: 1.000000, Valid Loss: 0.055970, Valid Acc: 0.988850, \n",
      "Epoch 37. Train Loss: 0.000198, Train Acc: 1.000000, Valid Loss: 0.056211, Valid Acc: 0.988850, \n",
      "Epoch 38. Train Loss: 0.000186, Train Acc: 1.000000, Valid Loss: 0.056218, Valid Acc: 0.988850, \n",
      "Epoch 39. Train Loss: 0.000175, Train Acc: 1.000000, Valid Loss: 0.056416, Valid Acc: 0.988950, \n",
      "Epoch 40. Train Loss: 0.000168, Train Acc: 1.000000, Valid Loss: 0.056493, Valid Acc: 0.988850, \n",
      "Epoch 41. Train Loss: 0.000166, Train Acc: 1.000000, Valid Loss: 0.056973, Valid Acc: 0.988850, \n",
      "Epoch 42. Train Loss: 0.000166, Train Acc: 1.000000, Valid Loss: 0.057017, Valid Acc: 0.988650, \n",
      "Epoch 43. Train Loss: 0.000151, Train Acc: 1.000000, Valid Loss: 0.057466, Valid Acc: 0.988650, \n",
      "Epoch 44. Train Loss: 0.000144, Train Acc: 1.000000, Valid Loss: 0.057217, Valid Acc: 0.988950, \n",
      "Epoch 45. Train Loss: 0.000146, Train Acc: 1.000000, Valid Loss: 0.057539, Valid Acc: 0.988650, \n",
      "Epoch 46. Train Loss: 0.000137, Train Acc: 1.000000, Valid Loss: 0.057449, Valid Acc: 0.988750, \n",
      "Epoch 47. Train Loss: 0.000144, Train Acc: 1.000000, Valid Loss: 0.058103, Valid Acc: 0.988750, \n",
      "Epoch 48. Train Loss: 0.000130, Train Acc: 1.000000, Valid Loss: 0.057706, Valid Acc: 0.989000, \n",
      "Epoch 49. Train Loss: 0.000128, Train Acc: 1.000000, Valid Loss: 0.058169, Valid Acc: 0.988800, \n"
     ]
    }
   ],
   "source": [
    "# train1(学习率为0.1)\n",
    "if __name__ == '__main__':\n",
    "    net = DenseNet()   \n",
    "    batch_size = 1000 \n",
    "    traindata, validdata = train_dataset('train.csv')\n",
    "    trainiter = DataLoader(traindata, batch_size=batch_size,shuffle=True)\n",
    "    validiter = DataLoader(validdata, batch_size=batch_size,shuffle=True)\n",
    "    num_epochs = 50\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-2) #随机梯度下降\n",
    "    criterion = nn.CrossEntropyLoss() #loss为交叉熵\n",
    "    \n",
    "    train(net, trainiter, validiter, num_epochs, optimizer, criterion)\n",
    "    torch.save(net, 'net2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4f5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgq/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.730034, Train Acc: 0.727213, Valid Loss: 0.555673, Valid Acc: 0.828850, \n",
      "Epoch 1. Train Loss: 0.473184, Train Acc: 0.874275, Valid Loss: 0.391396, Valid Acc: 0.897800, \n",
      "Epoch 2. Train Loss: 0.343598, Train Acc: 0.907863, Valid Loss: 0.295517, Valid Acc: 0.916050, \n",
      "Epoch 3. Train Loss: 0.268481, Train Acc: 0.921750, Valid Loss: 0.240232, Valid Acc: 0.928300, \n",
      "Epoch 4. Train Loss: 0.221339, Train Acc: 0.935250, Valid Loss: 0.204111, Valid Acc: 0.943150, \n",
      "Epoch 5. Train Loss: 0.188543, Train Acc: 0.947062, Valid Loss: 0.178058, Valid Acc: 0.951000, \n",
      "Epoch 6. Train Loss: 0.164428, Train Acc: 0.954075, Valid Loss: 0.158812, Valid Acc: 0.956350, \n",
      "Epoch 7. Train Loss: 0.146125, Train Acc: 0.959512, Valid Loss: 0.144368, Valid Acc: 0.960600, \n",
      "Epoch 8. Train Loss: 0.131908, Train Acc: 0.963175, Valid Loss: 0.132644, Valid Acc: 0.964250, \n",
      "Epoch 9. Train Loss: 0.120712, Train Acc: 0.966387, Valid Loss: 0.123240, Valid Acc: 0.966350, \n",
      "Epoch 10. Train Loss: 0.111056, Train Acc: 0.968875, Valid Loss: 0.115654, Valid Acc: 0.968150, \n",
      "Epoch 11. Train Loss: 0.103282, Train Acc: 0.971162, Valid Loss: 0.109749, Valid Acc: 0.970300, \n",
      "Epoch 12. Train Loss: 0.096164, Train Acc: 0.973000, Valid Loss: 0.104678, Valid Acc: 0.971800, \n",
      "Epoch 13. Train Loss: 0.090359, Train Acc: 0.974625, Valid Loss: 0.098984, Valid Acc: 0.972800, \n",
      "Epoch 14. Train Loss: 0.084888, Train Acc: 0.975850, Valid Loss: 0.095075, Valid Acc: 0.973400, \n",
      "Epoch 15. Train Loss: 0.080000, Train Acc: 0.976962, Valid Loss: 0.092238, Valid Acc: 0.974400, \n",
      "Epoch 16. Train Loss: 0.075323, Train Acc: 0.978375, Valid Loss: 0.088132, Valid Acc: 0.974950, \n",
      "Epoch 17. Train Loss: 0.071112, Train Acc: 0.979662, Valid Loss: 0.085070, Valid Acc: 0.975450, \n",
      "Epoch 18. Train Loss: 0.067711, Train Acc: 0.980562, Valid Loss: 0.082356, Valid Acc: 0.976600, \n",
      "Epoch 19. Train Loss: 0.063930, Train Acc: 0.982025, Valid Loss: 0.079729, Valid Acc: 0.976900, \n",
      "Epoch 20. Train Loss: 0.060756, Train Acc: 0.982737, Valid Loss: 0.077551, Valid Acc: 0.977400, \n",
      "Epoch 21. Train Loss: 0.057769, Train Acc: 0.983537, Valid Loss: 0.075242, Valid Acc: 0.978300, \n",
      "Epoch 22. Train Loss: 0.054915, Train Acc: 0.984687, Valid Loss: 0.074916, Valid Acc: 0.978750, \n",
      "Epoch 23. Train Loss: 0.052097, Train Acc: 0.985675, Valid Loss: 0.071926, Valid Acc: 0.979800, \n",
      "Epoch 24. Train Loss: 0.049847, Train Acc: 0.986037, Valid Loss: 0.070236, Valid Acc: 0.980050, \n",
      "Epoch 25. Train Loss: 0.046749, Train Acc: 0.987200, Valid Loss: 0.068698, Valid Acc: 0.980550, \n",
      "Epoch 26. Train Loss: 0.044931, Train Acc: 0.987862, Valid Loss: 0.067408, Valid Acc: 0.980250, \n",
      "Epoch 27. Train Loss: 0.042487, Train Acc: 0.988775, Valid Loss: 0.066193, Valid Acc: 0.981150, \n",
      "Epoch 28. Train Loss: 0.040166, Train Acc: 0.989337, Valid Loss: 0.065163, Valid Acc: 0.981500, \n",
      "Epoch 29. Train Loss: 0.038246, Train Acc: 0.990087, Valid Loss: 0.063838, Valid Acc: 0.981700, \n",
      "Epoch 30. Train Loss: 0.036715, Train Acc: 0.990325, Valid Loss: 0.063433, Valid Acc: 0.981750, \n",
      "Epoch 31. Train Loss: 0.034936, Train Acc: 0.991100, Valid Loss: 0.062053, Valid Acc: 0.982200, \n",
      "Epoch 32. Train Loss: 0.032845, Train Acc: 0.991988, Valid Loss: 0.060782, Valid Acc: 0.982800, \n",
      "Epoch 33. Train Loss: 0.031472, Train Acc: 0.992275, Valid Loss: 0.060032, Valid Acc: 0.982400, \n",
      "Epoch 34. Train Loss: 0.029939, Train Acc: 0.992962, Valid Loss: 0.059460, Valid Acc: 0.982350, \n",
      "Epoch 35. Train Loss: 0.028356, Train Acc: 0.993262, Valid Loss: 0.058411, Valid Acc: 0.982900, \n",
      "Epoch 36. Train Loss: 0.027022, Train Acc: 0.993700, Valid Loss: 0.058135, Valid Acc: 0.982800, \n",
      "Epoch 37. Train Loss: 0.025774, Train Acc: 0.994275, Valid Loss: 0.057375, Valid Acc: 0.983100, \n",
      "Epoch 38. Train Loss: 0.024378, Train Acc: 0.994725, Valid Loss: 0.056621, Valid Acc: 0.983150, \n",
      "Epoch 39. Train Loss: 0.023132, Train Acc: 0.995100, Valid Loss: 0.056546, Valid Acc: 0.983450, \n",
      "Epoch 40. Train Loss: 0.022012, Train Acc: 0.995550, Valid Loss: 0.055437, Valid Acc: 0.983350, \n",
      "Epoch 41. Train Loss: 0.020972, Train Acc: 0.995587, Valid Loss: 0.055133, Valid Acc: 0.983250, \n",
      "Epoch 42. Train Loss: 0.019875, Train Acc: 0.996200, Valid Loss: 0.054265, Valid Acc: 0.983700, \n",
      "Epoch 43. Train Loss: 0.019144, Train Acc: 0.996425, Valid Loss: 0.054379, Valid Acc: 0.983850, \n",
      "Epoch 44. Train Loss: 0.018016, Train Acc: 0.996862, Valid Loss: 0.053768, Valid Acc: 0.983850, \n",
      "Epoch 45. Train Loss: 0.017067, Train Acc: 0.997087, Valid Loss: 0.053666, Valid Acc: 0.984250, \n",
      "Epoch 46. Train Loss: 0.016394, Train Acc: 0.997163, Valid Loss: 0.053289, Valid Acc: 0.984000, \n",
      "Epoch 47. Train Loss: 0.015442, Train Acc: 0.997612, Valid Loss: 0.052812, Valid Acc: 0.984200, \n",
      "Epoch 48. Train Loss: 0.014879, Train Acc: 0.997563, Valid Loss: 0.052650, Valid Acc: 0.983900, \n",
      "Epoch 49. Train Loss: 0.014235, Train Acc: 0.997800, Valid Loss: 0.052017, Valid Acc: 0.984550, \n",
      "Epoch 50. Train Loss: 0.013616, Train Acc: 0.998000, Valid Loss: 0.052264, Valid Acc: 0.984600, \n",
      "Epoch 51. Train Loss: 0.012911, Train Acc: 0.998037, Valid Loss: 0.052180, Valid Acc: 0.984450, \n",
      "Epoch 52. Train Loss: 0.012442, Train Acc: 0.998212, Valid Loss: 0.051706, Valid Acc: 0.984750, \n",
      "Epoch 53. Train Loss: 0.011992, Train Acc: 0.998388, Valid Loss: 0.051386, Valid Acc: 0.985050, \n",
      "Epoch 54. Train Loss: 0.011591, Train Acc: 0.998487, Valid Loss: 0.051434, Valid Acc: 0.984400, \n",
      "Epoch 55. Train Loss: 0.010989, Train Acc: 0.998537, Valid Loss: 0.051471, Valid Acc: 0.985000, \n",
      "Epoch 56. Train Loss: 0.010372, Train Acc: 0.998687, Valid Loss: 0.051583, Valid Acc: 0.984800, \n",
      "Epoch 57. Train Loss: 0.010116, Train Acc: 0.998700, Valid Loss: 0.050676, Valid Acc: 0.985250, \n",
      "Epoch 58. Train Loss: 0.009731, Train Acc: 0.998850, Valid Loss: 0.051378, Valid Acc: 0.985200, \n",
      "Epoch 59. Train Loss: 0.009182, Train Acc: 0.998950, Valid Loss: 0.051298, Valid Acc: 0.985250, \n",
      "Epoch 60. Train Loss: 0.008823, Train Acc: 0.999100, Valid Loss: 0.050837, Valid Acc: 0.985400, \n",
      "Epoch 61. Train Loss: 0.008609, Train Acc: 0.999062, Valid Loss: 0.050766, Valid Acc: 0.985000, \n",
      "Epoch 62. Train Loss: 0.008124, Train Acc: 0.999238, Valid Loss: 0.050745, Valid Acc: 0.985300, \n",
      "Epoch 63. Train Loss: 0.007949, Train Acc: 0.999150, Valid Loss: 0.051114, Valid Acc: 0.985150, \n",
      "Epoch 64. Train Loss: 0.007572, Train Acc: 0.999312, Valid Loss: 0.050924, Valid Acc: 0.985500, \n",
      "Epoch 65. Train Loss: 0.007306, Train Acc: 0.999287, Valid Loss: 0.050631, Valid Acc: 0.985300, \n",
      "Epoch 66. Train Loss: 0.007158, Train Acc: 0.999237, Valid Loss: 0.050837, Valid Acc: 0.985600, \n",
      "Epoch 67. Train Loss: 0.006679, Train Acc: 0.999437, Valid Loss: 0.050659, Valid Acc: 0.985400, \n",
      "Epoch 68. Train Loss: 0.006550, Train Acc: 0.999412, Valid Loss: 0.050533, Valid Acc: 0.985600, \n",
      "Epoch 69. Train Loss: 0.006281, Train Acc: 0.999463, Valid Loss: 0.051226, Valid Acc: 0.985450, \n",
      "Epoch 70. Train Loss: 0.006232, Train Acc: 0.999463, Valid Loss: 0.051230, Valid Acc: 0.985900, \n",
      "Epoch 71. Train Loss: 0.006021, Train Acc: 0.999488, Valid Loss: 0.051045, Valid Acc: 0.986350, \n",
      "Epoch 72. Train Loss: 0.005680, Train Acc: 0.999550, Valid Loss: 0.051017, Valid Acc: 0.985650, \n",
      "Epoch 73. Train Loss: 0.005658, Train Acc: 0.999512, Valid Loss: 0.050978, Valid Acc: 0.985800, \n",
      "Epoch 74. Train Loss: 0.005503, Train Acc: 0.999537, Valid Loss: 0.050889, Valid Acc: 0.985850, \n",
      "Epoch 75. Train Loss: 0.005299, Train Acc: 0.999575, Valid Loss: 0.050683, Valid Acc: 0.985650, \n",
      "Epoch 76. Train Loss: 0.005128, Train Acc: 0.999625, Valid Loss: 0.050671, Valid Acc: 0.985900, \n",
      "Epoch 77. Train Loss: 0.004931, Train Acc: 0.999650, Valid Loss: 0.050686, Valid Acc: 0.986000, \n",
      "Epoch 78. Train Loss: 0.004906, Train Acc: 0.999600, Valid Loss: 0.050685, Valid Acc: 0.985900, \n",
      "Epoch 79. Train Loss: 0.004847, Train Acc: 0.999650, Valid Loss: 0.050951, Valid Acc: 0.986150, \n",
      "Epoch 80. Train Loss: 0.004565, Train Acc: 0.999713, Valid Loss: 0.051074, Valid Acc: 0.986450, \n",
      "Epoch 81. Train Loss: 0.004433, Train Acc: 0.999712, Valid Loss: 0.051108, Valid Acc: 0.986050, \n",
      "Epoch 82. Train Loss: 0.004445, Train Acc: 0.999725, Valid Loss: 0.051037, Valid Acc: 0.985700, \n",
      "Epoch 83. Train Loss: 0.004139, Train Acc: 0.999750, Valid Loss: 0.050985, Valid Acc: 0.986300, \n",
      "Epoch 84. Train Loss: 0.004116, Train Acc: 0.999800, Valid Loss: 0.051241, Valid Acc: 0.986050, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85. Train Loss: 0.003885, Train Acc: 0.999812, Valid Loss: 0.050616, Valid Acc: 0.986000, \n",
      "Epoch 86. Train Loss: 0.003963, Train Acc: 0.999800, Valid Loss: 0.051200, Valid Acc: 0.986400, \n",
      "Epoch 87. Train Loss: 0.003729, Train Acc: 0.999850, Valid Loss: 0.051050, Valid Acc: 0.986350, \n",
      "Epoch 88. Train Loss: 0.003607, Train Acc: 0.999850, Valid Loss: 0.051454, Valid Acc: 0.986250, \n",
      "Epoch 89. Train Loss: 0.003513, Train Acc: 0.999900, Valid Loss: 0.051474, Valid Acc: 0.986250, \n",
      "Epoch 90. Train Loss: 0.003498, Train Acc: 0.999875, Valid Loss: 0.051534, Valid Acc: 0.986000, \n",
      "Epoch 91. Train Loss: 0.003319, Train Acc: 0.999912, Valid Loss: 0.051506, Valid Acc: 0.985850, \n",
      "Epoch 92. Train Loss: 0.003336, Train Acc: 0.999912, Valid Loss: 0.051133, Valid Acc: 0.986100, \n",
      "Epoch 93. Train Loss: 0.003351, Train Acc: 0.999875, Valid Loss: 0.051841, Valid Acc: 0.986000, \n",
      "Epoch 94. Train Loss: 0.003176, Train Acc: 0.999925, Valid Loss: 0.051304, Valid Acc: 0.985950, \n",
      "Epoch 95. Train Loss: 0.003112, Train Acc: 0.999888, Valid Loss: 0.051365, Valid Acc: 0.986000, \n",
      "Epoch 96. Train Loss: 0.002995, Train Acc: 0.999925, Valid Loss: 0.051276, Valid Acc: 0.986550, \n",
      "Epoch 97. Train Loss: 0.002928, Train Acc: 0.999912, Valid Loss: 0.051576, Valid Acc: 0.986250, \n",
      "Epoch 98. Train Loss: 0.002937, Train Acc: 0.999925, Valid Loss: 0.051632, Valid Acc: 0.986350, \n",
      "Epoch 99. Train Loss: 0.002806, Train Acc: 0.999925, Valid Loss: 0.051237, Valid Acc: 0.986450, \n"
     ]
    }
   ],
   "source": [
    "# train2(学习率为0.01)\n",
    "if __name__ == '__main__':\n",
    "    net = DenseNet()   \n",
    "    batch_size = 1000 \n",
    "    traindata, validdata = train_dataset('train.csv')\n",
    "    trainiter = DataLoader(traindata, batch_size=batch_size,shuffle=True)\n",
    "    validiter = DataLoader(validdata, batch_size=batch_size,shuffle=True)\n",
    "    num_epochs = 100\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-2) #随机梯度下降\n",
    "    criterion = nn.CrossEntropyLoss() #loss为交叉熵\n",
    "    \n",
    "    train(net, trainiter, validiter, num_epochs, optimizer, criterion)\n",
    "    torch.save(net, 'net2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999018e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgq/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1.0000e+00, 2.5332e-07, 5.9304e-09, 2.4673e-08],\n",
      "        [1.6729e-07, 7.2411e-06, 9.9999e-01, 9.4169e-10],\n",
      "        [4.0069e-08, 1.4034e-06, 8.3287e-08, 1.0000e+00],\n",
      "        ...,\n",
      "        [1.5378e-04, 1.7448e-05, 9.9983e-01, 5.2307e-07],\n",
      "        [1.8216e-06, 9.9998e-01, 1.6172e-05, 3.8846e-06],\n",
      "        [9.9999e-01, 8.8189e-06, 3.3051e-07, 1.2294e-06]]), tensor([[1.0000e+00, 1.4064e-08, 7.9501e-08, 3.9498e-08],\n",
      "        [8.9368e-03, 2.0326e-04, 9.9019e-01, 6.7218e-04],\n",
      "        [9.9968e-01, 3.2479e-04, 5.4699e-08, 1.3502e-07],\n",
      "        ...,\n",
      "        [2.4143e-06, 5.1156e-06, 1.3137e-05, 9.9998e-01],\n",
      "        [9.9215e-01, 1.0123e-04, 7.6869e-03, 6.5770e-05],\n",
      "        [1.0000e+00, 4.7768e-08, 1.7844e-08, 8.5085e-08]]), tensor([[1.2245e-05, 1.7748e-05, 9.9996e-01, 7.1848e-06],\n",
      "        [1.4035e-05, 2.6831e-06, 9.9998e-01, 7.2340e-08],\n",
      "        [1.0000e+00, 2.9090e-07, 2.1095e-09, 2.5298e-08],\n",
      "        ...,\n",
      "        [9.0497e-07, 2.3294e-06, 5.3225e-05, 9.9994e-01],\n",
      "        [5.9161e-05, 1.4291e-03, 3.3894e-06, 9.9851e-01],\n",
      "        [9.9996e-01, 1.8262e-05, 1.8835e-05, 6.0059e-06]]), tensor([[2.9533e-06, 2.7244e-05, 9.9996e-01, 1.2954e-05],\n",
      "        [9.9998e-01, 1.1333e-05, 1.1685e-05, 8.4649e-07],\n",
      "        [1.3421e-08, 1.3509e-07, 3.2797e-08, 1.0000e+00],\n",
      "        ...,\n",
      "        [1.0000e+00, 9.7168e-09, 3.8363e-09, 1.9569e-09],\n",
      "        [7.8394e-09, 1.6311e-07, 1.4663e-08, 1.0000e+00],\n",
      "        [9.9999e-01, 8.3087e-06, 6.5450e-07, 1.3286e-06]]), tensor([[1.0154e-07, 1.6758e-07, 1.0000e+00, 6.3911e-09],\n",
      "        [5.0691e-06, 1.1194e-05, 9.9998e-01, 5.0139e-06],\n",
      "        [9.1772e-07, 6.1096e-06, 9.1341e-06, 9.9998e-01],\n",
      "        ...,\n",
      "        [1.0592e-08, 6.4811e-08, 5.3256e-09, 1.0000e+00],\n",
      "        [1.0000e+00, 2.3072e-10, 5.9851e-12, 1.5536e-11],\n",
      "        [1.0000e+00, 1.0476e-07, 3.5833e-10, 6.4444e-09]]), tensor([[1.1144e-08, 5.3311e-08, 1.1080e-09, 1.0000e+00],\n",
      "        [9.9998e-01, 1.3498e-05, 1.7119e-06, 2.0432e-06],\n",
      "        [9.9999e-01, 8.2433e-07, 1.2066e-05, 2.2097e-07],\n",
      "        ...,\n",
      "        [4.8651e-07, 1.5468e-06, 1.7771e-05, 9.9998e-01],\n",
      "        [1.0000e+00, 8.4856e-10, 1.6616e-11, 5.6207e-10],\n",
      "        [1.0000e+00, 3.3897e-06, 6.9922e-08, 3.6187e-07]]), tensor([[1.0000e+00, 2.7646e-09, 1.5948e-10, 2.7759e-10],\n",
      "        [9.9999e-01, 3.0647e-06, 8.6788e-06, 4.0550e-07],\n",
      "        [3.5265e-07, 3.3909e-06, 5.3771e-06, 9.9999e-01],\n",
      "        ...,\n",
      "        [9.8966e-01, 1.0335e-02, 5.6912e-06, 2.1151e-06],\n",
      "        [1.5487e-07, 1.4704e-06, 3.9299e-07, 1.0000e+00],\n",
      "        [4.7040e-08, 2.9240e-07, 2.2394e-08, 1.0000e+00]]), tensor([[9.9999e-01, 1.1774e-05, 7.9160e-07, 5.1894e-07],\n",
      "        [1.0000e+00, 4.4537e-09, 3.4293e-11, 1.0068e-09],\n",
      "        [9.9997e-01, 2.9731e-05, 4.9990e-07, 4.6279e-07],\n",
      "        ...,\n",
      "        [4.0591e-06, 7.7578e-06, 9.9999e-01, 2.6386e-06],\n",
      "        [1.0000e+00, 6.1446e-07, 7.4644e-08, 2.4156e-07],\n",
      "        [5.8950e-07, 7.2824e-07, 1.1330e-07, 1.0000e+00]]), tensor([[1.0000e+00, 1.9536e-07, 6.1696e-08, 2.2626e-08],\n",
      "        [9.9994e-01, 5.5295e-05, 1.4960e-08, 4.9777e-08],\n",
      "        [1.0000e+00, 9.7917e-07, 7.0365e-09, 8.6186e-08],\n",
      "        ...,\n",
      "        [6.1765e-03, 1.1815e-02, 2.8245e-04, 9.8173e-01],\n",
      "        [9.9841e-01, 1.7308e-04, 7.4658e-05, 1.3394e-03],\n",
      "        [8.2388e-05, 1.8024e-05, 9.9989e-01, 6.6006e-06]]), tensor([[9.9995e-01, 4.0277e-05, 8.8150e-06, 1.5329e-06],\n",
      "        [3.3137e-05, 3.7735e-06, 9.9996e-01, 4.8998e-08],\n",
      "        [3.5864e-04, 1.5111e-02, 9.8428e-01, 2.5257e-04],\n",
      "        ...,\n",
      "        [1.0000e+00, 1.8325e-06, 2.6361e-07, 9.1997e-08],\n",
      "        [7.2253e-07, 6.4499e-06, 3.2189e-06, 9.9999e-01],\n",
      "        [1.0000e+00, 2.6039e-08, 2.3586e-09, 5.1053e-09]]), tensor([[9.9999e-01, 1.1722e-05, 1.0667e-07, 2.4770e-07],\n",
      "        [9.9991e-01, 7.1814e-05, 2.1356e-05, 9.3198e-07],\n",
      "        [1.3120e-06, 2.7407e-06, 1.0000e+00, 1.1621e-07],\n",
      "        ...,\n",
      "        [1.0000e+00, 5.3988e-08, 4.4694e-09, 2.0762e-09],\n",
      "        [1.0000e+00, 1.2628e-06, 1.8791e-08, 5.4048e-08],\n",
      "        [1.0000e+00, 4.2312e-07, 3.1495e-07, 2.7232e-07]]), tensor([[9.9991e-01, 2.9356e-05, 1.4656e-07, 5.8468e-05],\n",
      "        [1.0000e+00, 1.0437e-08, 2.8529e-11, 9.6741e-11],\n",
      "        [1.0000e+00, 3.9241e-06, 1.1317e-07, 4.9440e-07],\n",
      "        ...,\n",
      "        [1.0000e+00, 6.7405e-07, 3.9293e-08, 2.3712e-08],\n",
      "        [7.9113e-07, 1.0000e+00, 2.1660e-07, 3.3226e-09],\n",
      "        [1.0000e+00, 9.5168e-10, 1.3864e-11, 2.3980e-10]]), tensor([[1.5859e-07, 4.2358e-07, 9.4327e-07, 1.0000e+00],\n",
      "        [1.0000e+00, 1.2853e-08, 9.6191e-09, 4.1368e-09],\n",
      "        [1.0000e+00, 7.9251e-08, 2.1377e-09, 1.7582e-08],\n",
      "        ...,\n",
      "        [1.0000e+00, 6.0561e-09, 1.8986e-10, 9.3340e-08],\n",
      "        [1.0000e+00, 1.6365e-09, 1.7432e-10, 2.0076e-11],\n",
      "        [9.9999e-01, 3.8888e-06, 1.3746e-06, 3.4078e-07]]), tensor([[1.0000e+00, 1.2839e-12, 1.6747e-14, 1.9984e-13],\n",
      "        [9.5605e-07, 7.5007e-06, 9.9999e-01, 5.3424e-07],\n",
      "        [8.4360e-07, 2.5226e-07, 1.0000e+00, 2.0662e-08],\n",
      "        ...,\n",
      "        [1.6826e-07, 9.9996e-01, 2.3986e-05, 1.2134e-05],\n",
      "        [9.9999e-01, 5.7380e-06, 6.2293e-07, 2.2204e-07],\n",
      "        [7.6957e-05, 9.9989e-01, 1.2959e-06, 3.4568e-05]]), tensor([[8.8926e-01, 6.0719e-02, 4.3409e-02, 6.6129e-03],\n",
      "        [6.0602e-06, 5.4347e-05, 9.9993e-01, 5.0474e-06],\n",
      "        [6.3873e-09, 5.8761e-06, 9.9999e-01, 4.0075e-07],\n",
      "        ...,\n",
      "        [1.0297e-05, 9.4828e-06, 9.9996e-01, 1.8258e-05],\n",
      "        [1.0000e+00, 6.0371e-08, 1.7153e-08, 5.8608e-08],\n",
      "        [9.9999e-01, 5.8676e-06, 2.3653e-08, 5.6493e-07]]), tensor([[3.2021e-06, 2.0796e-05, 5.6767e-05, 9.9992e-01],\n",
      "        [4.7886e-08, 9.7427e-08, 7.3556e-09, 1.0000e+00],\n",
      "        [1.0000e+00, 1.5969e-09, 8.1497e-12, 8.1526e-11],\n",
      "        ...,\n",
      "        [5.6947e-07, 1.0969e-06, 2.3417e-06, 1.0000e+00],\n",
      "        [9.9966e-01, 9.0427e-05, 1.3937e-04, 1.1462e-04],\n",
      "        [7.5546e-09, 3.2802e-08, 8.4417e-09, 1.0000e+00]]), tensor([[3.6312e-05, 2.2130e-05, 9.9994e-01, 1.3229e-06],\n",
      "        [1.0000e+00, 1.9092e-06, 1.8874e-07, 1.6175e-07],\n",
      "        [2.0515e-06, 1.0954e-05, 7.6313e-06, 9.9998e-01],\n",
      "        ...,\n",
      "        [1.0000e+00, 1.2578e-07, 1.9858e-08, 4.5334e-08],\n",
      "        [1.0000e+00, 1.2298e-07, 4.5151e-09, 5.6699e-09],\n",
      "        [1.0000e+00, 4.7156e-08, 2.3342e-08, 1.8826e-08]]), tensor([[1.0000e+00, 3.2641e-09, 2.9248e-09, 3.7543e-09],\n",
      "        [1.0000e+00, 2.0119e-08, 2.6246e-09, 1.3908e-09],\n",
      "        [7.5818e-01, 2.4161e-01, 7.7345e-05, 1.3623e-04],\n",
      "        ...,\n",
      "        [1.3745e-07, 7.0179e-07, 1.4765e-07, 1.0000e+00],\n",
      "        [1.2756e-08, 5.2646e-07, 3.1247e-08, 1.0000e+00],\n",
      "        [4.7722e-08, 1.0379e-06, 5.8271e-06, 9.9999e-01]]), tensor([[1.0000e+00, 1.7614e-06, 3.0450e-08, 5.7296e-08],\n",
      "        [1.0000e+00, 6.3480e-09, 2.9985e-10, 1.5646e-09],\n",
      "        [1.0000e+00, 6.7372e-11, 9.6731e-13, 2.0963e-12],\n",
      "        ...,\n",
      "        [1.9260e-04, 8.3929e-06, 9.9979e-01, 8.5705e-06],\n",
      "        [1.0000e+00, 1.0953e-07, 2.2172e-08, 1.5516e-08],\n",
      "        [9.9963e-01, 1.1378e-05, 1.9409e-04, 1.6753e-04]]), tensor([[9.2568e-07, 1.0000e+00, 3.0036e-07, 9.5522e-07],\n",
      "        [1.0000e+00, 1.1742e-07, 9.9918e-09, 4.6196e-08],\n",
      "        [5.2327e-08, 1.9716e-07, 2.9455e-09, 1.0000e+00],\n",
      "        ...,\n",
      "        [1.1943e-03, 6.2208e-05, 9.9872e-01, 2.6094e-05],\n",
      "        [1.0000e+00, 2.3617e-07, 3.7280e-08, 3.8387e-08],\n",
      "        [8.5702e-01, 3.8612e-02, 1.2656e-02, 9.1709e-02]])]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    model = torch.load('net1.pth')\n",
    "    model = model.to(device)\n",
    "    model.eval()  # 转为test模式\n",
    "    batch_size = 1000\n",
    "    testdata = test_dataset('testA.csv')\n",
    "    testiter = DataLoader(testdata, batch_size=batch_size,shuffle=False) # 一定要定义为False!\n",
    "#     print(iter(testiter).next().shape)\n",
    "    result = []\n",
    "    for data in testiter:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        pre = F.softmax(out, 1)\n",
    "        pre = pre.to('cpu')\n",
    "#         print(pre)\n",
    "        result.append(pre)\n",
    "    print(result)\n",
    "    result = torch.stack(result, 0) #按照轴0将list转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc7d08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>119995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>119996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>119997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>119998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>119999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label_0  label_1  label_2  label_3\n",
       "0      100000      1.0      0.0      0.0      0.0\n",
       "1      100001      0.0      0.0      1.0      0.0\n",
       "2      100002      0.0      0.0      0.0      1.0\n",
       "3      100003      1.0      0.0      0.0      0.0\n",
       "4      100004      1.0      0.0      0.0      0.0\n",
       "...       ...      ...      ...      ...      ...\n",
       "19995  119995      1.0      0.0      0.0      0.0\n",
       "19996  119996      1.0      0.0      0.0      0.0\n",
       "19997  119997      0.0      0.0      1.0      0.0\n",
       "19998  119998      1.0      0.0      0.0      0.0\n",
       "19999  119999      1.0      0.0      0.0      0.0\n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行数据的后处理，准备提交数据(设置阈值)\n",
    "result = np.array(result)\n",
    "result = result.reshape((20000,4))\n",
    "thr = [0.7, 0.35, 0.45, 0.5]\n",
    "for x in result:\n",
    "    for i in [1, 2, 3, 0]:\n",
    "        if x[i] > thr[i]:\n",
    "            x[0:i] = 0\n",
    "            x[i+1:4] = 0\n",
    "            x[i] = 1\n",
    "            break\n",
    "\n",
    "id =np.arange(100000,120000)\n",
    "df = DataFrame(result, columns=['label_0','label_1','label_2','label_3'])\n",
    "df.insert(loc=0, column='id', value=id, allow_duplicates=False) \n",
    "df.to_csv(\"submit.csv\", index_label=\"id\", index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd5fe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>119995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>119996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>119997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>119998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>119999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label_0  label_1  label_2  label_3\n",
       "0      100000      1.0      0.0      0.0      0.0\n",
       "1      100001      0.0      0.0      1.0      0.0\n",
       "2      100002      0.0      0.0      0.0      1.0\n",
       "3      100003      1.0      0.0      0.0      0.0\n",
       "4      100004      1.0      0.0      0.0      0.0\n",
       "...       ...      ...      ...      ...      ...\n",
       "19995  119995      1.0      0.0      0.0      0.0\n",
       "19996  119996      1.0      0.0      0.0      0.0\n",
       "19997  119997      0.0      0.0      1.0      0.0\n",
       "19998  119998      1.0      0.0      0.0      0.0\n",
       "19999  119999      1.0      0.0      0.0      0.0\n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行数据的后处理，准备提交数据(直接取最大值)\n",
    "result = np.array(result)\n",
    "result = result.reshape((20000,4))\n",
    "for x in result:\n",
    "    for i in range(4):\n",
    "        if x[i] is max(x):\n",
    "            x[0:i] = 0\n",
    "            x[i+1:4] = 0\n",
    "            x[i] = 1\n",
    "            break\n",
    "\n",
    "id =np.arange(100000,120000)\n",
    "df = DataFrame(result, columns=['label_0','label_1','label_2','label_3'])\n",
    "df.insert(loc=0, column='id', value=id, allow_duplicates=False) \n",
    "df.to_csv(\"submit1.csv\", index_label=\"id\", index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a0ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
